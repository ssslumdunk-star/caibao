#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ•°æ®ç¼“å­˜ç®¡ç†å™¨
å®ç°è´¢æŠ¥æ•°æ®çš„æœ¬åœ°ç¼“å­˜å­˜å‚¨ï¼Œå‡å°‘APIè°ƒç”¨é¢‘ç‡
"""

import json
import os
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
import logging
from pathlib import Path

logger = logging.getLogger('DataCacheManager')

@dataclass
class CachedEarningsEvent:
    """ç¼“å­˜çš„è´¢æŠ¥äº‹ä»¶"""
    symbol: str
    company_name: str
    earnings_date: str
    earnings_time: str
    quarter: str
    fiscal_year: int
    eps_estimate: Optional[float] = None
    eps_actual: Optional[float] = None
    revenue_estimate: Optional[float] = None
    revenue_actual: Optional[float] = None
    beat_estimate: Optional[bool] = None
    last_updated: str = ""
    data_source: str = "yahoo_finance"

@dataclass
class CachedAnalystData:
    """ç¼“å­˜çš„åˆ†æå¸ˆæ•°æ®"""
    symbol: str
    current_price: float
    target_mean: Optional[float] = None
    target_high: Optional[float] = None
    target_low: Optional[float] = None
    recommendation_key: Optional[str] = None
    analyst_count: Optional[int] = None
    last_updated: str = ""
    data_source: str = "yahoo_finance"

class DataCacheManager:
    """æ•°æ®ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "data/cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        self.db_path = self.cache_dir / "earnings_cache.db"
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self._init_database()
        
        # ç¼“å­˜é…ç½®
        self.cache_config = {
            'earnings_cache_days': 1,      # è´¢æŠ¥æ•°æ®ç¼“å­˜1å¤©
            'analyst_cache_hours': 6,      # åˆ†æå¸ˆæ•°æ®ç¼“å­˜6å°æ—¶  
            'prediction_cache_days': 7,    # é¢„æµ‹æ•°æ®ç¼“å­˜7å¤©
            'max_cache_entries': 10000     # æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
        }
    
    def _init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
        with sqlite3.connect(self.db_path) as conn:
            # è´¢æŠ¥äº‹ä»¶è¡¨
            conn.execute('''
                CREATE TABLE IF NOT EXISTS earnings_events (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    symbol TEXT NOT NULL,
                    company_name TEXT,
                    earnings_date TEXT NOT NULL,
                    earnings_time TEXT,
                    quarter TEXT,
                    fiscal_year INTEGER,
                    eps_estimate REAL,
                    eps_actual REAL,
                    revenue_estimate REAL,
                    revenue_actual REAL,
                    beat_estimate INTEGER,
                    last_updated TEXT NOT NULL,
                    data_source TEXT NOT NULL,
                    UNIQUE(symbol, earnings_date)
                )
            ''')
            
            # åˆ†æå¸ˆæ•°æ®è¡¨
            conn.execute('''
                CREATE TABLE IF NOT EXISTS analyst_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    symbol TEXT NOT NULL,
                    current_price REAL NOT NULL,
                    target_mean REAL,
                    target_high REAL,
                    target_low REAL,
                    recommendation_key TEXT,
                    analyst_count INTEGER,
                    last_updated TEXT NOT NULL,
                    data_source TEXT NOT NULL,
                    UNIQUE(symbol)
                )
            ''')
            
            # åˆ›å»ºç´¢å¼•
            conn.execute('CREATE INDEX IF NOT EXISTS idx_earnings_symbol_date ON earnings_events(symbol, earnings_date)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_analyst_symbol ON analyst_data(symbol)')
            
            conn.commit()
            logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    
    def cache_earnings_events(self, events: List[CachedEarningsEvent]) -> int:
        """ç¼“å­˜è´¢æŠ¥äº‹ä»¶æ•°æ®"""
        if not events:
            return 0
            
        cached_count = 0
        current_time = datetime.now().isoformat()
        
        with sqlite3.connect(self.db_path) as conn:
            for event in events:
                event.last_updated = current_time
                
                try:
                    # ä½¿ç”¨ INSERT OR REPLACE æ¥å¤„ç†é‡å¤æ•°æ®
                    conn.execute('''
                        INSERT OR REPLACE INTO earnings_events 
                        (symbol, company_name, earnings_date, earnings_time, quarter, 
                         fiscal_year, eps_estimate, eps_actual, revenue_estimate, 
                         revenue_actual, beat_estimate, last_updated, data_source)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        event.symbol, event.company_name, event.earnings_date,
                        event.earnings_time, event.quarter, event.fiscal_year,
                        event.eps_estimate, event.eps_actual, event.revenue_estimate,
                        event.revenue_actual, event.beat_estimate, event.last_updated,
                        event.data_source
                    ))
                    cached_count += 1
                    
                except sqlite3.Error as e:
                    logger.error(f"ç¼“å­˜è´¢æŠ¥äº‹ä»¶å¤±è´¥ {event.symbol}: {e}")
                    
            conn.commit()
        
        logger.info(f"æˆåŠŸç¼“å­˜ {cached_count} ä¸ªè´¢æŠ¥äº‹ä»¶")
        return cached_count
    
    def get_cached_earnings_events(self, symbol: str = None, start_date: str = None, 
                                 end_date: str = None) -> List[CachedEarningsEvent]:
        """è·å–ç¼“å­˜çš„è´¢æŠ¥äº‹ä»¶"""
        query = "SELECT * FROM earnings_events WHERE 1=1"
        params = []
        
        if symbol:
            query += " AND symbol = ?"
            params.append(symbol)
            
        if start_date:
            query += " AND earnings_date >= ?"
            params.append(start_date)
            
        if end_date:
            query += " AND earnings_date <= ?"
            params.append(end_date)
            
        query += " ORDER BY earnings_date"
        
        events = []
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute(query, params)
            
            for row in cursor.fetchall():
                event = CachedEarningsEvent(
                    symbol=row['symbol'],
                    company_name=row['company_name'],
                    earnings_date=row['earnings_date'],
                    earnings_time=row['earnings_time'],
                    quarter=row['quarter'],
                    fiscal_year=row['fiscal_year'],
                    eps_estimate=row['eps_estimate'],
                    eps_actual=row['eps_actual'],
                    revenue_estimate=row['revenue_estimate'],
                    revenue_actual=row['revenue_actual'],
                    beat_estimate=bool(row['beat_estimate']) if row['beat_estimate'] is not None else None,
                    last_updated=row['last_updated'],
                    data_source=row['data_source']
                )
                events.append(event)
        
        return events
    
    def cache_analyst_data(self, analyst_data: CachedAnalystData) -> bool:
        """ç¼“å­˜åˆ†æå¸ˆæ•°æ®"""
        current_time = datetime.now().isoformat()
        analyst_data.last_updated = current_time
        
        with sqlite3.connect(self.db_path) as conn:
            try:
                conn.execute('''
                    INSERT OR REPLACE INTO analyst_data 
                    (symbol, current_price, target_mean, target_high, target_low,
                     recommendation_key, analyst_count, last_updated, data_source)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    analyst_data.symbol, analyst_data.current_price,
                    analyst_data.target_mean, analyst_data.target_high,
                    analyst_data.target_low, analyst_data.recommendation_key,
                    analyst_data.analyst_count, analyst_data.last_updated,
                    analyst_data.data_source
                ))
                
                conn.commit()
                logger.info(f"æˆåŠŸç¼“å­˜ {analyst_data.symbol} åˆ†æå¸ˆæ•°æ®")
                return True
                
            except sqlite3.Error as e:
                logger.error(f"ç¼“å­˜åˆ†æå¸ˆæ•°æ®å¤±è´¥ {analyst_data.symbol}: {e}")
                return False
    
    def get_cached_analyst_data(self, symbol: str) -> Optional[CachedAnalystData]:
        """è·å–ç¼“å­˜çš„åˆ†æå¸ˆæ•°æ®"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute(
                "SELECT * FROM analyst_data WHERE symbol = ?", (symbol,)
            )
            
            row = cursor.fetchone()
            if row:
                return CachedAnalystData(
                    symbol=row['symbol'],
                    current_price=row['current_price'],
                    target_mean=row['target_mean'],
                    target_high=row['target_high'],
                    target_low=row['target_low'],
                    recommendation_key=row['recommendation_key'],
                    analyst_count=row['analyst_count'],
                    last_updated=row['last_updated'],
                    data_source=row['data_source']
                )
        
        return None
    
    def is_cache_valid(self, last_updated: str, cache_hours: int = 24) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä»ç„¶æœ‰æ•ˆ"""
        try:
            cached_time = datetime.fromisoformat(last_updated)
            current_time = datetime.now()
            time_diff = current_time - cached_time
            
            return time_diff < timedelta(hours=cache_hours)
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæ€§å¤±è´¥: {e}")
            return False
    
    def clean_expired_cache(self):
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜æ•°æ®"""
        current_time = datetime.now()
        
        # æ¸…ç†è¿‡æœŸçš„è´¢æŠ¥äº‹ä»¶ (è¶…è¿‡30å¤©)
        cutoff_date = (current_time - timedelta(days=30)).isoformat()
        
        with sqlite3.connect(self.db_path) as conn:
            # æ¸…ç†è´¢æŠ¥äº‹ä»¶
            cursor = conn.execute(
                "DELETE FROM earnings_events WHERE last_updated < ?", (cutoff_date,)
            )
            earnings_deleted = cursor.rowcount
            
            # æ¸…ç†åˆ†æå¸ˆæ•°æ® (è¶…è¿‡7å¤©)
            analyst_cutoff = (current_time - timedelta(days=7)).isoformat()
            cursor = conn.execute(
                "DELETE FROM analyst_data WHERE last_updated < ?", (analyst_cutoff,)
            )
            analyst_deleted = cursor.rowcount
            
            conn.commit()
            
        logger.info(f"æ¸…ç†è¿‡æœŸç¼“å­˜: è´¢æŠ¥äº‹ä»¶ {earnings_deleted} æ¡, åˆ†æå¸ˆæ•°æ® {analyst_deleted} æ¡")
        
        return earnings_deleted + analyst_deleted
    
    def get_cache_stats(self) -> Dict[str, int]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        with sqlite3.connect(self.db_path) as conn:
            # è´¢æŠ¥äº‹ä»¶ç»Ÿè®¡
            cursor = conn.execute("SELECT COUNT(*) FROM earnings_events")
            stats['earnings_events'] = cursor.fetchone()[0]
            
            # åˆ†æå¸ˆæ•°æ®ç»Ÿè®¡
            cursor = conn.execute("SELECT COUNT(*) FROM analyst_data")
            stats['analyst_data'] = cursor.fetchone()[0]
            
            # æœ€æ–°æ›´æ–°æ—¶é—´
            cursor = conn.execute(
                "SELECT MAX(last_updated) FROM earnings_events"
            )
            result = cursor.fetchone()[0]
            stats['last_earnings_update'] = result or "æ— æ•°æ®"
            
            cursor = conn.execute(
                "SELECT MAX(last_updated) FROM analyst_data"
            )
            result = cursor.fetchone()[0]
            stats['last_analyst_update'] = result or "æ— æ•°æ®"
        
        return stats
    
    def export_cache_to_json(self, output_file: str = None) -> str:
        """å¯¼å‡ºç¼“å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = self.cache_dir / f"cache_export_{timestamp}.json"
        
        # è·å–æ‰€æœ‰æ•°æ®
        earnings_events = self.get_cached_earnings_events()
        
        # æ„å»ºå¯¼å‡ºæ•°æ®
        export_data = {
            'export_time': datetime.now().isoformat(),
            'earnings_events': [asdict(event) for event in earnings_events],
            'cache_stats': self.get_cache_stats()
        }
        
        # å†™å…¥JSONæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç¼“å­˜æ•°æ®å·²å¯¼å‡ºåˆ°: {output_file}")
        return str(output_file)
    
    def import_cache_from_json(self, input_file: str) -> bool:
        """ä»JSONæ–‡ä»¶å¯¼å…¥ç¼“å­˜æ•°æ®"""
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # å¯¼å…¥è´¢æŠ¥äº‹ä»¶
            if 'earnings_events' in data:
                events = []
                for event_data in data['earnings_events']:
                    event = CachedEarningsEvent(**event_data)
                    events.append(event)
                
                count = self.cache_earnings_events(events)
                logger.info(f"ä» {input_file} å¯¼å…¥äº† {count} ä¸ªè´¢æŠ¥äº‹ä»¶")
            
            return True
            
        except Exception as e:
            logger.error(f"å¯¼å…¥ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
            return False

# ä½¿ç”¨ç¤ºä¾‹å’Œå·¥å…·å‡½æ•°
def create_sample_data():
    """åˆ›å»ºä¸€äº›ç¤ºä¾‹ç¼“å­˜æ•°æ®"""
    cache_manager = DataCacheManager()
    
    # åˆ›å»ºç¤ºä¾‹è´¢æŠ¥äº‹ä»¶
    sample_events = [
        CachedEarningsEvent(
            symbol="AAPL",
            company_name="Apple Inc.",
            earnings_date="2024-01-25",
            earnings_time="AMC",
            quarter="Q1 2024",
            fiscal_year=2024,
            eps_estimate=2.10,
            eps_actual=2.18,
            revenue_estimate=117500000000,
            revenue_actual=119575000000,
            beat_estimate=True,
            data_source="manual_input"
        ),
        CachedEarningsEvent(
            symbol="MSFT", 
            company_name="Microsoft Corp.",
            earnings_date="2024-01-24",
            earnings_time="AMC",
            quarter="Q2 2024",
            fiscal_year=2024,
            eps_estimate=11.05,
            revenue_estimate=60000000000,
            data_source="manual_input"
        )
    ]
    
    # ç¼“å­˜ç¤ºä¾‹æ•°æ®
    cache_manager.cache_earnings_events(sample_events)
    
    # åˆ›å»ºåˆ†æå¸ˆæ•°æ®ç¤ºä¾‹
    analyst_data = CachedAnalystData(
        symbol="AAPL",
        current_price=230.50,
        target_mean=250.00,
        target_high=280.00,
        target_low=220.00,
        recommendation_key="buy",
        analyst_count=25,
        data_source="manual_input"
    )
    
    cache_manager.cache_analyst_data(analyst_data)
    
    return cache_manager

if __name__ == "__main__":
    # æ¼”ç¤ºç¼“å­˜ç®¡ç†å™¨åŠŸèƒ½
    print("ğŸ—„ï¸ æ•°æ®ç¼“å­˜ç®¡ç†å™¨æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºç¼“å­˜ç®¡ç†å™¨
    cache_manager = create_sample_data()
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = cache_manager.get_cache_stats()
    print("ğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    print()
    
    # è·å–ç¼“å­˜æ•°æ®ç¤ºä¾‹
    print("ğŸ“… è·å–AAPLè´¢æŠ¥äº‹ä»¶:")
    events = cache_manager.get_cached_earnings_events(symbol="AAPL")
    for event in events:
        print(f"  {event.earnings_date}: EPSé¢„æœŸ={event.eps_estimate}, å®é™…={event.eps_actual}")
    
    print()
    
    # è·å–åˆ†æå¸ˆæ•°æ®ç¤ºä¾‹
    print("ğŸ“ˆ è·å–AAPLåˆ†æå¸ˆæ•°æ®:")
    analyst = cache_manager.get_cached_analyst_data("AAPL")
    if analyst:
        print(f"  å½“å‰ä»·æ ¼: ${analyst.current_price}")
        print(f"  ç›®æ ‡ä»·æ ¼: ${analyst.target_mean}")
        print(f"  æ¨èç­‰çº§: {analyst.recommendation_key}")
    
    print()
    
    # å¯¼å‡ºæ•°æ®
    export_file = cache_manager.export_cache_to_json()
    print(f"ğŸ“¤ æ•°æ®å·²å¯¼å‡ºåˆ°: {export_file}")
    
    print("\nâœ… ç¼“å­˜ç®¡ç†å™¨åŠŸèƒ½æ­£å¸¸")